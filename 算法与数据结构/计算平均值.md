在spark中，计算Watermark的时候会跟踪数据的最大最小值，其中就用到了平均值的计算，感觉很方便，

代码如下：

```scala
case class EventTimeStats(var max: Long, var min: Long, var avg: Double, var count: Long) {
  def add(eventTime: Long): Unit = {
    this.max = math.max(this.max, eventTime)
    this.min = math.min(this.min, eventTime)
    this.count += 1
    this.avg += (eventTime - avg) / count
  }
```

计算平均值的时候不会用到累积的结果，只是在原来的均值和新的个数之上来计算，原理就是当前新值与之前旧的均值做差，得到了一个diff，然后再把这个diff平均到count上，这样就计算出了均值。